{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9afa537-74ad-4a96-8c46-ca43632f534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_dssat_pdi\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69682fe9-e80e-4b4d-8474-3639b6767b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"eval*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e101137-933c-4911-b0ad-f33a4011ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for fname in fnames:\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read()\n",
    "        data = data.strip().split(\"\\n\")\n",
    "        data = [d.split(':') for d in data]\n",
    "        data = {d[0].strip(): [float(x) for x in d[1].replace(' ', '').replace('[', '').replace(']', '').strip().split(',')] for d in data}\n",
    "        dfs[fname] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a53ad16-33ee-4a5d-b304-ac2eb7d98f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Agent</th>\n",
       "      <th>Expert Agent</th>\n",
       "      <th>PPO Agent</th>\n",
       "      <th>A2C Agent</th>\n",
       "      <th>DDPG Agent</th>\n",
       "      <th>TD3 Agent</th>\n",
       "      <th>SAC Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2253.889917</td>\n",
       "      <td>3912.057383</td>\n",
       "      <td>2891.311433</td>\n",
       "      <td>2253.889917</td>\n",
       "      <td>-7919.564761</td>\n",
       "      <td>-7919.564761</td>\n",
       "      <td>4428.500671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>392.639243</td>\n",
       "      <td>1776.285213</td>\n",
       "      <td>472.888459</td>\n",
       "      <td>392.639243</td>\n",
       "      <td>2675.743113</td>\n",
       "      <td>2675.743113</td>\n",
       "      <td>2121.116370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1356.660939</td>\n",
       "      <td>1350.649891</td>\n",
       "      <td>1865.380959</td>\n",
       "      <td>1356.660939</td>\n",
       "      <td>-13726.692526</td>\n",
       "      <td>-13726.692526</td>\n",
       "      <td>-1934.556005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1952.744966</td>\n",
       "      <td>2235.837298</td>\n",
       "      <td>2509.480403</td>\n",
       "      <td>1952.744966</td>\n",
       "      <td>-9881.046124</td>\n",
       "      <td>-9881.046124</td>\n",
       "      <td>3037.388995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2251.545415</td>\n",
       "      <td>3869.728190</td>\n",
       "      <td>2881.411294</td>\n",
       "      <td>2251.545415</td>\n",
       "      <td>-7807.065291</td>\n",
       "      <td>-7807.065291</td>\n",
       "      <td>5184.092860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2545.412841</td>\n",
       "      <td>5039.154600</td>\n",
       "      <td>3244.000219</td>\n",
       "      <td>2545.412841</td>\n",
       "      <td>-5908.974436</td>\n",
       "      <td>-5908.974436</td>\n",
       "      <td>6203.288115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3294.720838</td>\n",
       "      <td>8900.127178</td>\n",
       "      <td>3985.312240</td>\n",
       "      <td>3294.720838</td>\n",
       "      <td>-2894.025578</td>\n",
       "      <td>-2894.025578</td>\n",
       "      <td>7217.932737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Null Agent  Expert Agent    PPO Agent    A2C Agent    DDPG Agent  \\\n",
       "count   400.000000    400.000000   400.000000   400.000000    400.000000   \n",
       "mean   2253.889917   3912.057383  2891.311433  2253.889917  -7919.564761   \n",
       "std     392.639243   1776.285213   472.888459   392.639243   2675.743113   \n",
       "min    1356.660939   1350.649891  1865.380959  1356.660939 -13726.692526   \n",
       "25%    1952.744966   2235.837298  2509.480403  1952.744966  -9881.046124   \n",
       "50%    2251.545415   3869.728190  2881.411294  2251.545415  -7807.065291   \n",
       "75%    2545.412841   5039.154600  3244.000219  2545.412841  -5908.974436   \n",
       "max    3294.720838   8900.127178  3985.312240  3294.720838  -2894.025578   \n",
       "\n",
       "          TD3 Agent    SAC Agent  \n",
       "count    400.000000   400.000000  \n",
       "mean   -7919.564761  4428.500671  \n",
       "std     2675.743113  2121.116370  \n",
       "min   -13726.692526 -1934.556005  \n",
       "25%    -9881.046124  3037.388995  \n",
       "50%    -7807.065291  5184.092860  \n",
       "75%    -5908.974436  6203.288115  \n",
       "max    -2894.025578  7217.932737  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['eval_output_cotton_True_all_123.txt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca67ac0-ef4e-485f-a39f-1429420f387f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eval_output_maize_False_all_123.txt', 'eval_output_cotton_False_all_123.txt', 'eval_output_rice_True_all_123.txt', 'eval_output.txt', 'eval_output_maize_True_all_123.txt', 'eval_output_cotton_True_all_123.txt', 'eval_output_rice_False_all_123.txt'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "249fd43f-def2-4aa7-b584-affc45e86730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Null agent...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 173\u001b[0m\n\u001b[1;32m    171\u001b[0m null_agent \u001b[38;5;241m=\u001b[39m NullAgent(env)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating Null agent...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m null_returns \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnull_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 150\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(agent, n_episodes)\u001b[0m\n\u001b[1;32m    143\u001b[0m eval_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfertilization\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m456\u001b[39m,\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_weather\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m }\n\u001b[1;32m    148\u001b[0m env \u001b[38;5;241m=\u001b[39m Monitor(GymDssatWrapper(gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGymDssatPdi-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meval_args)))\n\u001b[0;32m--> 150\u001b[0m returns, _ \u001b[38;5;241m=\u001b[39m evaluate_policy(\n\u001b[1;32m    151\u001b[0m     agent, env, n_eval_episodes\u001b[38;5;241m=\u001b[39mn_episodes, return_episode_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    153\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returns\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_dssat_pdi\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# helpers for action normalization\n",
    "def normalize_action(action_space_limits, action):\n",
    "    \"\"\"Normalize the action from [low, high] to [-1, 1]\"\"\"\n",
    "    low, high = action_space_limits\n",
    "    return 2.0 * ((action - low) / (high - low)) - 1.0\n",
    "\n",
    "def denormalize_action(action_space_limits, action):\n",
    "    \"\"\"Denormalize the action from [-1, 1] to [low, high]\"\"\"\n",
    "    low, high = action_space_limits\n",
    "    return low + (0.5 * (action + 1.0) * (high - low))\n",
    "\n",
    "# Wrapper for easy and uniform interfacing with SB3\n",
    "class GymDssatWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(GymDssatWrapper, self).__init__(env)\n",
    "\n",
    "        self.action_low, self.action_high = self._get_action_space_bounds()\n",
    "\n",
    "        # using a normalized action space\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=\"float32\")\n",
    "\n",
    "        # using a vector representation of observations to allow\n",
    "        # easily using SB3 MlpPolicy\n",
    "        self.observation_space = gym.spaces.Box(low=0.0,\n",
    "                                                high=np.inf,\n",
    "                                                shape=env.observation_dict_to_array(\n",
    "                                                    env.observation).shape,\n",
    "                                                dtype=\"float32\"\n",
    "                                                )\n",
    "\n",
    "        # to avoid annoying problem with Monitor when episodes end and things are None\n",
    "        self.last_info = {}\n",
    "        self.last_obs = None\n",
    "\n",
    "    def _get_action_space_bounds(self):\n",
    "        box = self.env.action_space['anfer']\n",
    "        return box.low, box.high\n",
    "\n",
    "    def _format_action(self, action):\n",
    "        return { 'anfer': action[0] }\n",
    "\n",
    "    def _format_observation(self, observation):\n",
    "        return self.env.observation_dict_to_array(observation)\n",
    "\n",
    "    def reset(self):\n",
    "        return self._format_observation(self.env.reset())\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Rescale action from [-1, 1] to original action space interval\n",
    "        denormalized_action = denormalize_action((self.action_low, self.action_high), action)\n",
    "        formatted_action = self._format_action(denormalized_action)\n",
    "        obs, reward, done, info = self.env.step(formatted_action)\n",
    "\n",
    "        # handle `None`s in obs, reward, and info on done step\n",
    "        if done:\n",
    "            obs, reward, info = self.last_obs, 0, self.last_info\n",
    "        else:\n",
    "            self.last_obs = obs\n",
    "            self.last_info = info\n",
    "\n",
    "        formatted_observation = self._format_observation(obs)\n",
    "        return formatted_observation, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.env.set_seed(seed)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "# Create environment\n",
    "env_args = {\n",
    "    'mode': 'fertilization',\n",
    "    'seed': 123,\n",
    "    'random_weather': True,\n",
    "}\n",
    "\n",
    "env = GymDssatWrapper(gym.make('GymDssatPdi-v0', **env_args))\n",
    "\n",
    "# Training arguments for PPO agent\n",
    "ppo_args = {\n",
    "    'gamma': 1,\n",
    "    'learning_rate': 0.0003,\n",
    "    'seed': 123,\n",
    "}\n",
    "\n",
    "\n",
    "# Baseline agents for comparison\n",
    "class NullAgent:\n",
    "    \"\"\"\n",
    "    Agent always choosing to do no fertilization\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=None):\n",
    "        action = normalize_action((self.env.action_low, self.env.action_high), [0])\n",
    "        return np.array([action], dtype=np.float32), obs\n",
    "\n",
    "\n",
    "class ExpertAgent:\n",
    "    \"\"\"\n",
    "    Simple agent using policy of choosing fertilization amount based on days after planting\n",
    "    \"\"\"\n",
    "    fertilization_dic = {\n",
    "        40: 27,\n",
    "        45: 35,\n",
    "        80: 54,\n",
    "    }\n",
    "\n",
    "    def __init__(self, env, normalize_action=False, fertilization_dic=None):\n",
    "        self.env = env\n",
    "        self.normalize_action = normalize_action\n",
    "\n",
    "    def _policy(self, obs):\n",
    "        dap = int(obs[0][1])\n",
    "        return [self.fertilization_dic[dap] if dap in self.fertilization_dic else 0]\n",
    "\n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=None):\n",
    "        action = self._policy(obs)\n",
    "        action = normalize_action((self.env.action_low, self.env.action_high), action)\n",
    "        print(self.env.history)\n",
    "        return np.array([action], dtype=np.float32), obs\n",
    "\n",
    "\n",
    "# evaluation and plotting functions\n",
    "def evaluate(agent, n_episodes=10):\n",
    "    # Create eval env\n",
    "    eval_args = {\n",
    "        'mode': 'fertilization',\n",
    "        'seed': 456,\n",
    "        'random_weather': True,\n",
    "    }\n",
    "    env = Monitor(GymDssatWrapper(gym.make('GymDssatPdi-v0', **eval_args)))\n",
    "\n",
    "    returns, _ = evaluate_policy(\n",
    "        agent, env, n_eval_episodes=n_episodes, return_episode_rewards=True)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return returns\n",
    "\n",
    "def plot_results(labels, returns):\n",
    "    data_dict = {}\n",
    "    for label, data in zip(labels, returns):\n",
    "        data_dict[label] = data\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    ax = sns.boxplot(data=df)\n",
    "    ax.set_xlabel(\"policy\")\n",
    "    ax.set_ylabel(\"evaluation output\")\n",
    "    plt.savefig('results_sb3.pdf')\n",
    "    print(\"\\nThe result is saved in the current working directory as 'results_sb3.pdf'\\n\")\n",
    "    plt.show()\n",
    "\n",
    "# evaluate agents\n",
    "null_agent = NullAgent(env)\n",
    "print('Evaluating Null agent...')\n",
    "null_returns = evaluate(null_agent,n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f22b647d-dae6-4e98-9b50-0e07c6f2b376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cumsumfert': 0.0,\n",
       " 'dap': 4377089,\n",
       " 'dtt': 0.0,\n",
       " 'ep': 0.0,\n",
       " 'grnwt': 0.0,\n",
       " 'istage': 7,\n",
       " 'nstres': 0.0,\n",
       " 'swfac': 0.0,\n",
       " 'topwt': 0.0,\n",
       " 'vstage': 0.0,\n",
       " 'xlai': 0.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0591cabe-53d9-42f4-9511-14cb40ff71d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': [], 'action': [], 'reward': []}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d932d4-b826-40d7-9e74-217379cf0ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
